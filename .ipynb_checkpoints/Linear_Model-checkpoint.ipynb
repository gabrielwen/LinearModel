{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacky Boilerplates\n",
    "- Install `feast` with pip.\n",
    "- Activate user service account with credentials JSON.\n",
    "- Hacks to retrieve essential information for deployments and serving.\n",
    "\n",
    "**NOTE**: This code block might hangs for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feast.sdk.resources.entity import Entity\n",
    "from feast.sdk.resources.storage import Storage\n",
    "from feast.sdk.resources.feature import Feature, Datastore, ValueType\n",
    "from feast.sdk.resources.feature_set import FeatureSet, FileType\n",
    "import feast.specs.FeatureSpec_pb2 as feature_pb\n",
    "\n",
    "from feast.sdk.importer import Importer\n",
    "\n",
    "from feast.sdk.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the local Feast deployment\n",
    "# FEAST_CORE_URL = '10.148.0.46:30576'\n",
    "FEAST_CORE_URL = 'localhost:6565'\n",
    "STAGING_LOCATION = 'gs://kubecon-19-gojek/staging'\n",
    "fs = Client(core_url=FEAST_CORE_URL, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90aa9fd68ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_APPLICATION_CREDENTIALS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mcred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Install dependencies\n",
    "_ = subprocess.call(['pip', 'install', 'feast'], shell=True)\n",
    "# Retrieve user service account.\n",
    "_ = subprocess.call(['gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS'],\n",
    "                    shell=True)\n",
    "\n",
    "# Create KUBECONFIG. Use credential file to retrieve project/deployment names.\n",
    "cred_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "cred = {}\n",
    "with open(cred_path, 'r') as c:\n",
    "    cred = json.load(c)\n",
    "\n",
    "PROJECT = cred['project_id']\n",
    "APP_NAME = re.search('([a-z\\-]+)-user'.format(PROJECT),\n",
    "                     cred['client_email']).group(1)\n",
    "\n",
    "p = subprocess.Popen(['gcloud', 'container', 'clusters', 'list',\n",
    "                      '--filter', 'name=%s' % APP_NAME, '--format', 'json'],\n",
    "                    stdout=subprocess.PIPE)\n",
    "out, _ = p.communicate()\n",
    "config = json.loads(out)[0]\n",
    "ZONE = config['zone']\n",
    "\n",
    "print('PROJECT =', PROJECT)\n",
    "print('APP_NAME =', APP_NAME)\n",
    "print('ZONE =', ZONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load precomputed feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>datetime</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.166550</td>\n",
       "      <td>0.077912</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:01:00</td>\n",
       "      <td>0.350554</td>\n",
       "      <td>0.378997</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:02:00</td>\n",
       "      <td>0.922618</td>\n",
       "      <td>0.317972</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:03:00</td>\n",
       "      <td>0.065824</td>\n",
       "      <td>0.055651</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:04:00</td>\n",
       "      <td>0.847057</td>\n",
       "      <td>0.213472</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity            datetime  feature1  feature2  feature3  feature4\n",
       "0       0 2018-01-01 00:00:00  0.166550  0.077912         8         1\n",
       "1       0 2018-01-01 00:01:00  0.350554  0.378997         4         7\n",
       "2       0 2018-01-01 00:02:00  0.922618  0.317972         8         6\n",
       "3       0 2018-01-01 00:03:00  0.065824  0.055651         9         1\n",
       "4       0 2018-01-01 00:04:00  0.847057  0.213472         2        10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', names=['entity','datetime','feature1','feature2','feature3','feature4'], index_col=False)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register entity and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully applied entity with name: simple_entity\n",
      "---\n",
      "name: simple_entity\n",
      "description: nyc taxi dataset\n",
      "\n",
      "Successfully applied feature with id: simple_entity.datetime\n",
      "---\n",
      "id: simple_entity.datetime\n",
      "name: datetime\n",
      "owner: user@website.com\n",
      "description: nyc taxi dataset\n",
      "valueType: TIMESTAMP\n",
      "entity: simple_entity\n",
      "dataStores:\n",
      "  serving:\n",
      "    id: SERVING\n",
      "  warehouse:\n",
      "    id: WAREHOUSE\n",
      "\n",
      "Successfully applied feature with id: simple_entity.feature1\n",
      "---\n",
      "id: simple_entity.feature1\n",
      "name: feature1\n",
      "owner: user@website.com\n",
      "description: nyc taxi dataset\n",
      "valueType: DOUBLE\n",
      "entity: simple_entity\n",
      "dataStores:\n",
      "  serving:\n",
      "    id: SERVING\n",
      "  warehouse:\n",
      "    id: WAREHOUSE\n",
      "\n",
      "Successfully applied feature with id: simple_entity.feature2\n",
      "---\n",
      "id: simple_entity.feature2\n",
      "name: feature2\n",
      "owner: user@website.com\n",
      "description: nyc taxi dataset\n",
      "valueType: DOUBLE\n",
      "entity: simple_entity\n",
      "dataStores:\n",
      "  serving:\n",
      "    id: SERVING\n",
      "  warehouse:\n",
      "    id: WAREHOUSE\n",
      "\n",
      "Successfully applied feature with id: simple_entity.feature3\n",
      "---\n",
      "id: simple_entity.feature3\n",
      "name: feature3\n",
      "owner: user@website.com\n",
      "description: nyc taxi dataset\n",
      "valueType: INT64\n",
      "entity: simple_entity\n",
      "dataStores:\n",
      "  serving:\n",
      "    id: SERVING\n",
      "  warehouse:\n",
      "    id: WAREHOUSE\n",
      "\n",
      "Successfully applied feature with id: simple_entity.feature4\n",
      "---\n",
      "id: simple_entity.feature4\n",
      "name: feature4\n",
      "owner: user@website.com\n",
      "description: nyc taxi dataset\n",
      "valueType: INT64\n",
      "entity: simple_entity\n",
      "dataStores:\n",
      "  serving:\n",
      "    id: SERVING\n",
      "  warehouse:\n",
      "    id: WAREHOUSE\n",
      "\n",
      "Staging file to remote path gs://kubecon-19-gojek/staging/tmp_simple_entity_1557650700037.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-95d39113b996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Ingest the feature data into the store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_entity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/feast/sdk/client.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, importer, name_override, apply_entity, apply_features)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_staging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Staging file to remote path {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Submitting job with spec:\\n {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_to_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/feast/sdk/importer.py\u001b[0m in \u001b[0;36mstage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mts_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestampColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0m_convert_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mdf_to_gcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/feast/sdk/importer.py\u001b[0m in \u001b[0;36m_convert_timestamp\u001b[0;34m(df, timestamp_col)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"Converts the given df's timestamp column to ISO8601 format\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestamp_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestamp_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%dT%H:%M:%S%zZ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dt'"
     ]
    }
   ],
   "source": [
    "# from feast.sdk.resources.entity import Entity\n",
    "# from feast.sdk.resources.feature import Feature\n",
    "\n",
    "\n",
    "# # Register a simple entity\n",
    "# demo_entity = Entity(name='demo_entity', description='My simple demo entity')\n",
    "# fs.apply(demo_entity)\n",
    "\n",
    "# # Register five numeric features on this entity\n",
    "# from feast.sdk.resources.feature import Feature\n",
    "\n",
    "# my_simple\n",
    "\n",
    "# Now that we have finished creating our features, we ingest them into feast\n",
    "\n",
    "# Initialise client\n",
    "# fs = Client(core_url=FEAST_CORE_URL, verbose=True)\n",
    "\n",
    "serving_ds=Datastore(id='SERVING')\n",
    "warehouse_ds=Datastore(id='WAREHOUSE')\n",
    "\n",
    "# Create importer\n",
    "importer = Importer.from_df(df, \n",
    "                           entity='simple_entity', \n",
    "                           owner='user@website.com',  \n",
    "                           staging_location=STAGING_LOCATION,\n",
    "                           id_column='entity', \n",
    "                           timestamp_column='dt',\n",
    "                           serving_store=serving_ds,\n",
    "                           warehouse_store=warehouse_ds)\n",
    "\n",
    "# Update feature and entity metadata. Ideally you want to update these manually\n",
    "# so that they contain adequate information for the next user\n",
    "importer.entity.description = 'nyc taxi dataset' \n",
    "for feature_id in importer.features:\n",
    "    importer.features[feature_id].description = 'nyc taxi dataset'\n",
    "    \n",
    "# Ingest the feature data into the store\n",
    "fs.run(importer, apply_features=True, apply_entity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'describe',\n",
       " 'dump',\n",
       " 'entity',\n",
       " 'features',\n",
       " 'from_bq',\n",
       " 'from_csv',\n",
       " 'from_df',\n",
       " 'remote_path',\n",
       " 'require_staging',\n",
       " 'size',\n",
       " 'source',\n",
       " 'spec',\n",
       " 'stage']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(Importer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64 53 27 30 74 90 50 62 64 62 35 46 51 22 41 89 85  5 61 40 52  6 40 13\n",
      " 86 68  1 52 51  9 46  5 64 81 50 59 80 52 38 53  7 89 66 69 61 99  2 94\n",
      " 73 55 14 77 41 79 89 71 89 32 91 26 52 96 31 26 13 70 92 78 31 46 10 45\n",
      " 64 69  5 15 61 99 95 82 57 19  3 64 90 44 49 89 49 82 55 90 27 18  1 55\n",
      " 14 91 48 62]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.random.randint(1, high=100, size=(200, 100))\n",
    "A = np.insert(train_data, 100, 1, axis=1)\n",
    "Y = np.random.randint(1, high=100, size=200)\n",
    "\n",
    "x = np.linalg.lstsq(A, Y, rcond=0)[0]\n",
    "m, b = x[:100], x[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_predict(x):\n",
    "    return m * x + b\n",
    "\n",
    "feature = np.random.randint(1, high=100, size=100)\n",
    "p = local_predict(feature)\n",
    "np.set_printoptions(precision=3)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "MODEL_FILE = 'simple_model.dat'\n",
    "\n",
    "model = {\n",
    "    'm': m.tolist(),\n",
    "    'b': b,\n",
    "}\n",
    "\n",
    "model_path = os.path.join(os.getenv('HOME', '/home'), MODEL_FILE)\n",
    "print('writing to', model_path)\n",
    "\n",
    "with open(model_path, 'w+') as f:\n",
    "    json.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy with Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairing\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job'.format(PROJECT)\n",
    "BASE_IMAGE = 'gcr.io/kubeflow-images-public/fairing-base:v20190510'\n",
    "SERVING_LABEL = 'kubeflow-fairing-demo'\n",
    "\n",
    "print('docker registry:', DOCKER_REGISTRY)\n",
    "print('base image:', BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deploy_with_fairing\n",
    "import uuid\n",
    "\n",
    "# To disambiguate between different deployments.\n",
    "serving_label = SERVING_LABEL + '-' + uuid.uuid4().hex[:4]\n",
    "print('Deploying service with selector', serving_label)\n",
    "\n",
    "# Register for docker credential. Needed for docker image pushes.\n",
    "_ = subprocess.call(['gcloud auth configure-docker --quiet'], shell=True)\n",
    "\n",
    "importlib.reload(deploy_with_fairing)\n",
    "deploy_with_fairing.deploy(DOCKER_REGISTRY, BASE_IMAGE, serving_label=serving_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from kubernetes import client, config\n",
    "\n",
    "# Need to set up KUBECONFIG. Kubernetes API client depends on it.\n",
    "subprocess.call(['gcloud', 'container', 'clusters', 'get-credentials', APP_NAME,\n",
    "                 '--zone', ZONE, '--project', PROJECT])\n",
    "config.load_kube_config()\n",
    "c = client.Configuration()\n",
    "client.Configuration.set_default(c)\n",
    "\n",
    "v1 = client.CoreV1Api()\n",
    "body = client.V1Service()\n",
    "label_selector = 'serving=%s' % serving_label\n",
    "resp = v1.list_service_for_all_namespaces(label_selector=label_selector)\n",
    "\n",
    "service_name = resp.items[0].metadata.name\n",
    "namespace = resp.items[0].metadata.namespace\n",
    "\n",
    "print('fairing service: {0}/{1}'.format(namespace, service_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def predict(url, data, feature_names=None):\n",
    "    pdata={\n",
    "        \"data\": {\n",
    "            \"names\":feature_names,\n",
    "            \"tensor\": {\n",
    "                \"shape\": np.asarray(data.shape).tolist(),\n",
    "                \"values\": data.flatten().tolist(),\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    serialized_data = json.dumps(pdata)\n",
    "    r = requests.post(url, data={'json':serialized_data})\n",
    "    return r\n",
    "\n",
    "def extract_prediction_array(content):\n",
    "    c = json.loads(content)\n",
    "    return np.array(c.get('data', {}).get('tensor', {}).get('values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "url = \"http://{service_name}.{namespace}.svc.cluster.local:5000/predict\".format(\n",
    "    service_name=service_name,\n",
    "    namespace=namespace)\n",
    "\n",
    "data = np.random.randint(1, high=100, size=100)\n",
    "r = predict(url, data)\n",
    "\n",
    "prediction = extract_prediction_array(r.content)\n",
    "print('prediction:')\n",
    "pprint.pprint(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
